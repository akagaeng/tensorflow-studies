{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Linear Regression cost 함수 최소화\n",
    "\n",
    "- [강의 동영상](https://www.youtube.com/watch?v=TxIVr-nk1so)\n",
    "- [강의 슬라이드](https://hunkim.github.io/ml/lec3.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 어떻게 cost function을 어떻게 최소화해서 최종적으로 linear regression 학습을 완성하는지 알아봄\n",
    "\n",
    "$$H(x) = Wx + b$$\n",
    "\n",
    "$$cost(W) = \\frac{1}{m}\\sum_{i=1}^{m} (W_x ^{(i)} - y^{(i)})^2$$\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "원래의 식에서 간략화를 위해 `b=0`이라고 가정하고 cost function을 살펴보자\n",
    "\n",
    "$H(x) = Wx$ ~~(+ b)~~\n",
    "\n",
    "$cost(W) = \\frac{1}{m}\\sum_{i=1}^{m} (W_x ^{(i)} - y^{(i)})^2$\n",
    "\n",
    "| x | y |\n",
    "|-------|\n",
    "| 1 | 1 |\n",
    "| 2 | 2 |\n",
    "| 3 | 3 |\n",
    "\n",
    "x, y가 위 표와 같은 경우 cost function 계산(참고: $ x^{(i)}|_{i=3}=3 $)\n",
    "- W=1\n",
    "\n",
    "> $\\frac{1}{3}\\{(1*1-1)^2 + (1*2-2)^2 + (1*3-3)^2\\} = 0$\n",
    "\n",
    "> $cost(1) = 0$\n",
    "\n",
    "- W=0\n",
    "\n",
    "> $\\frac{1}{3}\\{(0*1-1)^2 + (0*2-2)^2 + (0*3-3)^2\\} = \\frac{14}{3}$\n",
    "\n",
    "> $cost(0) = 4.67 $\n",
    "\n",
    "- W=2\n",
    "\n",
    "> $\\frac{1}{3}\\{(2*1-1)^2 + (2*2-2)^2 + (2*3-3)^2\\} = \\frac{14}{3}$\n",
    "\n",
    "> $cost(2) = 4.67 $\n",
    "\n",
    "\n",
    "이런 방식으로 그래프를 그려보면 다음과 같다.\n",
    "\n",
    "![images/lec3/W_cost_graph.png](images/lec3/W_cost_graph.png)\n",
    "- 이 그래프에서 x축은 W, y축은 cost인데, 이 때 cost가 최소화 되는 즉, cost가 0에 가까워지는 W를 구하는 것이 목적이다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent\n",
    "\n",
    "- minimization problem을 해결하는 데 많이 사용되는 알고리즘\n",
    "- cost function을 최소화하는 W, b를 구하는 데 사용된다.\n",
    "- W뿐만 아니라 여러 개의 W(W1, W2, ...)를 minimize하는 데에도 사용이 가능함\n",
    "\n",
    "### Gradient descent: How it works?\n",
    "- 임의의 값(W, b)에서 시작해서 cost(W,b)를 구한다.\n",
    "- W값을 조금씩 변화시켜가면서 cost(W,b)를 체크한다.\n",
    "- 다음 W값을 선택할 때에는 경사도(gradient)가 줄어드는 방향의 값을 선택한다.\n",
    "- 위 과정을 반복하면 cost(W,b)가 최소화되는 점에 도달하게 된다. (예외 있음)\n",
    "- 경사도는 \"미분\"을 이용하여 구한다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient descent algorithm\n",
    "- cost function을 미분하여 구함\n",
    "\n",
    "#### Cost function\n",
    "$$cost(W) = \\frac{1}{m}\\sum_{i=1}^{m} (W_x ^{(i)} - y^{(i)})^2$$\n",
    "- 이 cost function를 미분하여 Gradient descent algorithm을 구한다.\n",
    "\n",
    "#### Gradient descent algorithm\n",
    "$$W := W - \\alpha\\frac{1}{m}\\sum_{i=1}^{m} (Wx^{(i)} - y^{(i)})*x^{(i)}$$\n",
    "- 이 알고리즘을 이용하여 cost function을 최소화하는 W를 구할 수 있다.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convex function\n",
    "- cost function cost(W,b)을 (W,b)에 대해 그리면 그래프가 3차원으로 나타나게 된다.\n",
    "- 그래프의 형태가 여러 가지로 나올 수 있는데 다음 그림들을 살펴보자\n",
    "![images/lec3/multiple-destination.png](images/lec3/multiple-destination.png)\n",
    "- gradient descent algorithm을 적용하여 경사가 줄어드는 방향으로 점을 이동시킬 때 시작점이 어디냐에 따라서 ***다른 도착지에 도달***할 수 있다.\n",
    "- 따라서 cost minimize하는 W, b 값을 ***정확하게 찾을 수 없다***.\n",
    "\n",
    "![images/lec3/one-destination.png](images/lec3/one-destination.png)\n",
    "- gradient descent algorithm을 적용하여 경사가 줄어드는 방향으로 점을 이동시킬 때 시작점이 어디냐에 상관없이 ***같은 도착지에 도달***할 수 있다.\n",
    "- 따라서 cost minimize하는 W, b 값을 ***항상 정확하게 찾을 수 있다***.\n",
    "- 이러한 형태의 함수를 convex function이라 하며, linear regression 전에 cost function이 convex function 형태인지 먼저 확인하여야 한다!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
