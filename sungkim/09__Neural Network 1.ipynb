{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 9. Neural Network 1: XOR 문제와 학습방법, Backpropagation (1986 breakthrough)\n",
    "- [강의 동영상 9-1](https://youtu.be/GYecDQQwTdI): XOR 문제 딥러닝으로 풀기\n",
    "- [강의 동영상 9-2](https://youtu.be/573EZkzfnZ0): 딥넷트웍 학습 시키기 (backpropagation)\n",
    "- [강의 슬라이드 9-1, 9-2](https://hunkim.github.io/ml/lec9.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### XOR 문제\n",
    "- 하나의 logistic regression unit으로는 XOR을 나눌 수 **없다!**\n",
    "- Multiple logistic regression units로는 XOR을 나눌 수 **있다!**\n",
    "  + 그런데, layer가 많아질수록 각 게이트의 weight와 bias를 학습할 수가 없다.\n",
    "  + 어떻게 학습할 수 있을까?\n",
    "  \n",
    "#### XOR using NN(Neural Networks)\n",
    "- 네모 각각을 게이트, 퍼셉트론, 또는 유닛이라고 부름\n",
    "- 이 유닛들을 3개를 연결해서 XOR 문제를 해결할 수 있는지 살펴본다.\n",
    "\n",
    "#### 퍼셉트론 3개를 연결\n",
    "![forward-propagation-perceptron.png](images/lec9/forward-propagation-perceptron.png)\n",
    "\n",
    "#### 퍼셉트론 3개에서 최종적으로 나온 결과\n",
    "![forward-propagation-calculation.png](images/lec9/forward-propagation-calculation.png)\n",
    "\n",
    "- $x_1$과 $x_2$의 XOR 결과를 XOR에 나타내었고, 최종 결과인 $\\bar{y}$과 모두 동일하므로 퍼셉트론을 연결하여 XOR 문제를 해결할 수 있음을 살펴보았다.\n",
    "\n",
    "#### 퍼셉트론 여러 개를 하나로 표현\n",
    "- Multinomial classification에서처럼 여러개의 matrix를 하나로 합쳐서 표현할 수 있다.\n",
    "![merge-perceptrons.png](images/lec9/merge-perceptrons.png)\n",
    "\n",
    "- 이를 수식으로 표현해보면 다음과 같다.\n",
    "\n",
    "$$\n",
    "K(X) = sigmoid(XW_1 + B_1)\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\bar{Y} = H(X) = sigmoid(K(X)W_2+b_2)\n",
    "$$\n",
    "\n",
    "- 이를 tensorflow로 구현해보면 다음과 같다\n",
    "```python\n",
    "# Neural Network\n",
    "K = tf.sigmoid(tf.matmul(X, W1) + b1)\n",
    "hypothesis = tf.sigmoid(tf.matmul(K, W2) + b2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### training data로부터 W1, W2, B1, B2를 어떻게 구할까?\n",
    "- Gradient Descent 알고리즘 사용! ($y=x^2$ 꼴=convex function인 경우)\n",
    "\n",
    "#### 참고: concave and convex function\n",
    "![concave-convex-function.png](images/lec9/concave-convex-function.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "- input과 output 사이의 관계를 알기 위해서는 여러개의 신경망 layer의 각각의 unit마다 미분값을 구해야 하며, 이를 구하는 것은 수학적으로 어려우며 계산량이 너무나 많음\n",
    "- 이를 해결하기 위해 backpropagation이라는 방법이 고안됨\n",
    "- Paul Werbos(1974, 1982), Hinton(1986)\n",
    "\n",
    "![backpropagation.png](images/lec9/backpropagation.png)\n",
    "\n",
    "- 예측한 값과 실제 값 사이의 차이에서 나오는 오류(error) 즉, cost를 뒤에서부터 앞으로 전달하며 계산을 하도록 함\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backpropagation 구해보기(chain rule을 이용)\n",
    "\n",
    "![backpropagation-calculation.png](images/lec9/backpropagation-calculation.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. forward propagation 계산\n",
    "  - **입력 데이터의 값(w, x, b)으로 g, f를 구하는 것이 목적**\n",
    "  - w=-2, x=5, b=3\n",
    "  - $g = w \\cdot x = -2 \\cdot 5 = -10$\n",
    "  - $f = g + b = -10 + 3 = -7$\n",
    "\n",
    "2. backward propagation 계산\n",
    "  - 1의 결과로 $(1)\\frac{\\partial f}{\\partial b}, (2) \\frac{\\partial f}{\\partial w}, (3)\\frac{\\partial f}{\\partial x}$ 를 구하는 것이 목적\n",
    "  - (1) $f = g + b$ 를 $b$ 로 미분하면, $\\frac{\\partial f}{\\partial b} = 1$\n",
    "  - (2)(3): **Chain rule 적용**\n",
    "\n",
    "  $$\\frac{\\partial f}{\\partial x} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial x}$$\n",
    "  \n",
    "  $$\\frac{\\partial f}{\\partial w} = \\frac{\\partial f}{\\partial g} \\cdot \\frac{\\partial g}{\\partial w}$$\n",
    "  $$$$\n",
    "  \n",
    "  - 이 때, $f = g + b$ 를 $g$로 미분하면, $\\frac{\\partial f}{\\partial g} = 1$\n",
    "  - $g = wx$ 를 $x$로 미분하면, $\\frac{\\partial g}{\\partial x} = w$\n",
    "  - $g = wx$ 를 $w$로 미분하면, $\\frac{\\partial g}{\\partial w} = x$ 이므로\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  $$\\frac{\\partial f}{\\partial x} = 1 \\cdot w = w = -2$$\n",
    "  \n",
    "  $$\\frac{\\partial f}{\\partial w} = 1 \\cdot x = x = 5$$\n",
    "  \n",
    "이와 같이 계산이 가능하다!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이를 차근차근 해보면 아래와 같이 신경망의 layer가 더 많은 경우에도 계산할 수가 있다.\n",
    "\n",
    "![backpropagation-deep.png](images/lec9/backpropagation-deep.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### sigmoid 함수의 미분\n",
    "\n",
    "$$\n",
    "g(z) = \\frac{1}{1+e^{-z}}\n",
    "$$\n",
    "\n",
    "이 함수도 그래프로해서 chain rule로 미분값을 구할 수 있다.\n",
    "\n",
    "tensorflow에서도 이와같은 방법으로 그래프를 이용하여 backpropagation을 구하게 된다.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
