{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10. Neural Network 2: ReLU and 초기값 정하기 (2006/2007 breakthrough)\n",
    "- [강의 동영상 10-1](https://youtu.be/cKtg_fpw88c): XSigmoid 보다 ReLU가 더 좋아\n",
    "- [강의 동영상 10-2](https://youtu.be/4rC0sWrp3Uw): Weight 초기화 잘해보자\n",
    "- [강의 동영상 10-3](https://youtu.be/wTxMsp22llc): Dropout 과 앙상블\n",
    "- [강의 동영상 10-4](https://youtu.be/YHsbHjTBx9Q): 레고처럼 넷트웍 모듈을 마음껏 쌓아 보자\n",
    "- [강의 슬라이드 10-1, 10-2, 10-3, 10-4](https://hunkim.github.io/ml/lec10.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Let's go deep & wide!\n",
    "- sigmoid 함수와 같은 함수를 activate function이라고 한다.\n",
    "- 일정 조건을 갖추면 active가 되고 그렇지 않으면 active가 되지 않는다.\n",
    "\n",
    "#### (참고) Layer의 구분\n",
    "> **input layer | hiden layer(여러 층으로 구성) | output layer**\n",
    "\n",
    "### Vanishing gradient\n",
    "- Backpropagation의 문제\n",
    "- 2, 3단 정도는 학습이 잘 되나, 그 이상으로 연결이 깊어질수록(hidden layer가 많아질수록) 정확도가 높아지지 않음\n",
    "- chain rule를 이용해서 계산해보면 뒤에서부터 앞으로 오면서 값을 곱하게 되는데, 이 때 곱해지는 y 값은 ***sigmoid function*** 으로 계산된 값인데, 0~1 사이의 값이다. Layer가 늘어갈수록 더 많아지므로 곱해지는 항이 많아질수록 0에 가까운 값이 나오게 된다!\n",
    "\n",
    "### Vanishing gradient 문제 해결!\n",
    "- sigmoid 함수의 문제는 1보다 작은 값이 나온다는 것\n",
    "- 값이 너무 작아지지 않도록 새로운 함수를 사용하자! (by Hinton)\n",
    "\n",
    "#### ReLU\n",
    "- Rectified Linear Unit\n",
    "- 0보다 작을 때는 0, 0보다 크면 최대값 없이 증가!\n",
    "- 따라서 무조건 1보다 작은 값이 나오지 않는다.\n",
    "- Neural network에서는 더이상 sigmoid를 쓰지 않고 Relu를 사용!\n",
    "\n",
    "#### Sigmoid vs Relu\n",
    "![sigmoid-relu.png](images/lec10/sigmoid-relu.png)\n",
    "\n",
    "#### Activation function: Sigmoid\n",
    "![activation-function-sigmoid.png](images/lec10/activation-function-sigmoid.png)\n",
    "\n",
    "#### Activation function: Relu\n",
    "![activation-function-relu.png](images/lec10/activation-function-relu.png)\n",
    "\n",
    "#### Tensorflow로 구현\n",
    "![change-sigmoid-into-relu.png](images/lec10/change-sigmoid-into-relu.png)\n",
    "\n",
    "### Activate function의 종류 및 성능\n",
    "#### Activate Functions\n",
    "![activate-functions.png](images/lec10/activate-functions.png)\n",
    "\n",
    "#### Activation function accuracy\n",
    "![activation-function-accuracy.png](images/lec10/activation-function-accuracy.png)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 초기값 설정하는 방법!\n",
    "- 딥러닝을 잘하려면 초기값 설정을 잘 해야 한다.\n",
    "- 아래 그림을 살펴보면 activation function이 ReLU로 동일한 경우에도 cost function이 하나는 빨리 떨어지고, 하나는 느리게 떨어진다. ***why?*** **initial weight!**\n",
    "\n",
    "![cost-function-relu-initial-weight.png](images/lec10/cost-function-relu-initial-weight.png)\n",
    "\n",
    "- 실험에서 w 값을 -1~1 사이의 랜덤값을 부여하였다. 이 값에 따라 cost function의 변화가 달라진다.\n",
    "- 그렇다면 w를 몇을 주어야 할까?\n",
    "  + w=0인 경우, 학습이 전혀 되지 않는다!\n",
    "  + Hinton et al.(2006) \"A Fast Learning Algorithm for Deep Belif Nets\" 논문에서 이 방법을 찾기 위해 RBM(Restricted Boltzmann Machine)을 사용하였음\n",
    "  + RBM을 사용하여 최적화를시킨 Neural Network를 \"Deep Belif Nets\"라고 부른다.\n",
    "\n",
    "### RBM\n",
    "- 처음 2개의 레이어만 선택하여 Forward(encode), Backward(decode) 를 거치며 최적의 w를 선택\n",
    "- 이후 2개의 레이어를 선택하여 같은 방법을 사용\n",
    "- 끝에 갈 때까지 반복\n",
    "- 이를 이용하여 w값을 다 정해놓은 후에는 label을 통해 학습을 얼마 안시켜도 결과가 잘 나왔기 때문에, label을 이용한 학습 과정을 fine tuning이라고 불렀다.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Good news\n",
    "- RBM 자체가 매우 복잡한데, 이를 적용하지 않고도 weight initialization하는 방법을 발견\n",
    "\n",
    "Xavier initialization: X. Glorot and Y. Bengio, “Understanding the difficulty of\n",
    "training deep feedforward neural networks,” in International conference on artificial\n",
    "intelligence and statistics, 2010\n",
    "- 입력과 출력이 몇개냐에 따라서 초기값을 세팅하면 된다!\n",
    "\n",
    "He’s initialization: K. He, X. Zhang, S. Ren, and J. Sun, “Delving Deep into Rectifiers:\n",
    "Surpassing Human-Level Performance on ImageNet Classification,” 2015\n",
    "- 입력갯수, 출력갯수에 따라 랜덤한 숫자를 추고 입력값의 제곱근으로 나누어주니 잘 되더라.\n",
    "\n",
    "### Still an active area of research\n",
    "- 아직 정확히 어떻게 w를 초기화하는 것이 좋은지 밝혀지지 않았다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN dropout and model ensemble(앙상블)\n",
    "\n",
    "### Overfitting?\n",
    "- training 데이터셋에서는 정확도가 예를들어 0.99처럼 매우 높게 나오는데, test 데이터셋으로 확인해보면 0.85처럼 상대적으로 매우 낮게 나오는 경우\n",
    "- 실전에 사용하기 어려움. 응용력이 없다.\n",
    "\n",
    "### Overfitting 방지법\n",
    "- more training data\n",
    "- ~~reduce the number of features~~(딥러닝에서는 이건 상관없음!)\n",
    "- **Regularization!**\n",
    "\n",
    "### Regularization\n",
    "- weight에 너무 큰 숫자를 사용하지 말자.\n",
    "- $cost + \\lambda \\sum W^2$\n",
    "  + L2 regularization(w를 작게 조정)\n",
    "  + tensorflow example) L2Reg = 0.001 * tf.reduce_sum(tf.square(W))\n",
    "\n",
    "- Dropout: NN에서 추가되는 방법\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dropout\n",
    "- random하게 선택된 뉴런을 건너뛰고 학습시켜보자\n",
    "- Dropout: A Simple Way to Prevent Neural Networks\n",
    "from Overfitting [Srivastava et al. 2014]\n",
    "\n",
    "![neural-network-dropout.png](images/lec10/neural-network-dropout.png)\n",
    "\n",
    "- 언뜻 보면 좋은아이디어같지 않은데 왜 좋은 아이디어일까?\n",
    "![why-dropout-is-good.png](images/lec10/why-dropout-is-good.png)\n",
    "- 각 특징을 나타내는 뉴런 중 일부를 deactive시킨 후에 전체 특징으로 찾아보면 학습이 잘 되는 것을 알 수 있다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model ensemble(앙상블)\n",
    "- 학습시킬 수 있는 장비가 많은 경우 활용\n",
    "- 각각 다른 장비에서 다른 w 값으로 같거나 다른 트레이닝 세트로 학습을 시키면 결과값이 조금씩 다르게 나오게 된다.\n",
    "- 이렇게 나온 결과를 합쳐서 활용해보면 최소 2% 이상 정확도 향상 효과를 볼 수 있다!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Lego Play\n",
    "\n",
    "### Feedforward neural network\n",
    "- 레고를 쌓듯이 단을 쌓아서 neural network를 생성하는 방법\n",
    "- Fast forward, Split & merge, Recurrent network, ...\n",
    "- 여러 방법을 조합하여 network 생성\n",
    "- 상상하는대로 만들어서 조합하여 사용\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
